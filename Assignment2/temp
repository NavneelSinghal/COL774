training vectors: 4500
testing vectors: 1000
Linear Kernel results:
     pcost       dcost       gap    pres   dres
 0: -3.2916e+02 -9.3878e+03  5e+04  3e+00  5e-12
 1: -2.0018e+02 -5.2391e+03  1e+04  4e-01  5e-12
 2: -9.3627e+01 -1.6503e+03  3e+03  8e-02  4e-12
 3: -5.1870e+01 -7.1316e+02  1e+03  3e-02  2e-12
 4: -2.9393e+01 -4.7263e+02  7e+02  2e-02  2e-12
 5: -1.8616e+01 -2.5482e+02  4e+02  7e-03  1e-12
 6: -1.5942e+01 -8.1551e+01  9e+01  2e-03  9e-13
 7: -1.7889e+01 -4.1759e+01  3e+01  4e-04  8e-13
 8: -1.9475e+01 -3.1857e+01  1e+01  8e-05  9e-13
 9: -1.9419e+01 -2.8941e+01  1e+01  3e-14  9e-13
10: -2.1921e+01 -2.4878e+01  3e+00  1e-14  9e-13
11: -2.2590e+01 -2.3752e+01  1e+00  2e-15  1e-12
12: -2.3023e+01 -2.3203e+01  2e-01  2e-14  9e-13
13: -2.3099e+01 -2.3108e+01  1e-02  2e-14  1e-12
14: -2.3103e+01 -2.3103e+01  2e-04  1e-14  1e-12
15: -2.3103e+01 -2.3103e+01  2e-06  1e-14  1e-12
Optimal solution found.
b for sklearn: -1.4215423468767832
number of support vectors for sklearn: 198
support vectors: 198
w: [0.0, 0.0, -0.002, -0.005, 0.044, 0.05, 0.143, 0.372, 0.419, 0.594, 0.178, 0.03, 0.191, 0.836, 0.453, 0.15, 0.384, -0.188, 0.3, 0.048, 0.979, 0.466, 0.269, 0.108, -0.066, -0.015, -0.004, 0.0, 0.0, 0.0, -0.017, -0.056, -0.057, -0.1, 0.024, 0.03, -0.679, -0.474, -0.179, 0.519, 0.13, -0.24, 0.065, -0.328, -0.063, 0.2, 0.494, -0.044, 0.378, 0.58, 0.136, -0.174, -0.247, -0.177, -0.076, -0.045, 0.0, 0.0, -0.056, -0.138, -0.065, -0.174, -0.153, -0.332, -0.536, -0.157, -0.572, 0.36, 0.22, -0.167, -0.178, -0.26, 0.019, -0.086, 0.214, 0.154, -0.082, -0.198, -0.188, -0.168, -0.535, -0.2, -0.068, -0.027, 0.0, 0.005, -0.065, 0.021, 0.167, -0.104, -0.03, -0.281, -0.192, 0.119, -0.051, -0.172, 0.248, 0.32, 0.14, -0.071, 0.065, 0.411, 0.381, 0.24, 0.049, 0.131, -0.178, -0.163, -0.41, -0.305, -0.055, 0.0, 0.0, 0.004, -0.035, 0.122, 0.081, -0.088, 0.004, -0.245, 0.174, -0.134, 0.02, -0.033, 0.182, 0.292, 0.22, -0.25, 0.09, 0.166, -0.094, -0.043, -0.098, -0.062, -0.306, -0.049, -0.225, -0.319, -0.015, 0.003, 0.0, 0.0, 0.013, 0.058, 0.096, -0.11, 0.0, 0.264, 0.372, 0.114, -0.056, -0.179, -0.065, -0.143, 0.176, 0.56, -0.185, -0.131, -0.191, -0.119, -0.156, -0.148, -0.18, -0.08, -0.213, -0.327, -0.049, 0.006, 0.002, 0.008, 0.013, 0.016, 0.184, -0.239, 0.434, 0.167, 0.102, -0.223, 0.103, 0.111, -0.2, 0.455, 0.019, 0.879, -0.433, 0.552, -0.797, -0.451, 0.082, -0.111, -0.056, -0.072, -0.3, -0.365, -0.167, 0.004, 0.0, 0.044, -0.028, 0.067, 0.104, 0.054, 0.162, -0.227, -0.211, -0.042, -0.371, 0.184, 0.156, 0.123, 0.404, 0.054, -0.147, 0.08, -0.855, -0.132, 0.247, 0.014, 0.015, 0.015, -0.253, -0.229, -0.044, 0.01, 0.007, 0.039, 0.022, 0.047, -0.056, -0.036, 0.004, -0.305, -0.354, -0.09, 0.625, 0.225, 0.415, 0.151, 0.108, 0.126, -0.413, -0.048, -0.104, -0.038, -0.473, -0.237, -0.125, -0.055, -0.139, -0.207, -0.031, 0.004, 0.002, 0.06, 0.034, -0.108, -0.195, -0.383, -0.037, 0.0, -0.118, -0.275, 0.075, 0.165, 0.378, 0.363, 0.361, -0.25, 0.121, -0.068, 0.027, -0.221, -0.44, -0.253, -0.319, -0.013, -0.096, -0.223, -0.063, 0.007, 0.0, 0.067, 0.018, -0.223, -0.267, -0.26, -0.013, 0.135, 0.102, -0.063, -0.329, -0.003, -0.247, 0.476, 0.256, 0.226, 0.1, -0.555, 0.151, -0.226, -0.152, -0.113, -0.238, -0.156, -0.053, -0.162, -0.078, 0.007, 0.0, 0.071, -0.001, -0.167, -0.216, -0.209, 0.069, 0.166, 0.239, 0.645, 0.072, 0.219, 0.21, 0.191, -0.33, -0.166, 0.178, -0.026, 0.227, -0.46, 0.094, -0.009, -0.259, -0.105, -0.092, -0.001, -0.096, 0.008, 0.0, 0.068, -0.021, 0.047, -0.006, 0.147, 0.112, 0.196, 0.286, 0.558, 0.144, -0.451, 0.498, -0.29, -0.189, -0.119, -0.229, 0.0, 0.111, -0.227, 0.135, 0.028, 0.155, 0.182, -0.083, 0.086, -0.114, 0.006, 0.011, 0.079, -0.011, 0.048, 0.077, 0.142, 0.043, 0.052, 0.006, 0.246, 0.313, 0.272, 0.013, -0.35, -0.074, -0.14, -0.186, 0.304, -0.006, 0.032, 0.05, -0.148, 0.05, 0.096, -0.103, 0.11, -0.072, 0.001, 0.006, 0.083, -0.01, 0.031, 0.064, 0.076, -0.099, 0.181, -0.031, -0.103, -0.249, 0.008, 0.336, -0.109, -0.082, 0.326, 0.407, -0.187, -0.201, 0.209, 0.093, 0.033, 0.266, 0.117, 0.008, 0.047, -0.03, -0.016, -0.013, 0.092, -0.05, 0.075, 0.049, -0.002, 0.005, 0.119, 0.506, -0.491, -0.191, -0.199, 0.053, 0.19, -0.111, 0.229, 0.025, -0.11, -0.218, 0.14, 0.367, -0.015, 0.187, 0.062, 0.068, 0.068, -0.015, -0.028, 0.013, 0.153, -0.069, 0.054, 0.075, -0.218, -0.185, -0.011, 0.283, -0.216, 0.047, -0.414, 0.116, 0.413, 0.08, 0.228, -0.126, -0.031, -0.191, 0.04, 0.083, -0.007, 0.132, 0.002, 0.035, -0.016, 0.047, -0.032, 0.058, 0.263, -0.033, 0.098, 0.131, -0.353, -0.034, 0.082, -0.082, -0.236, 0.025, 0.036, -0.014, 0.119, -0.14, 0.291, 0.216, 0.419, 0.205, 0.106, 0.13, 0.118, 0.2, 0.141, 0.129, -0.037, 0.168, 0.0, 0.088, 0.261, -0.008, 0.11, 0.145, -0.313, 0.167, 0.179, 0.0, -0.491, 0.044, -0.243, -0.343, -0.262, -0.181, 0.124, 0.045, 0.278, 0.108, -0.007, 0.193, -0.012, 0.231, 0.083, 0.164, -0.065, 0.191, 0.02, 0.137, 0.236, 0.008, 0.094, 0.177, -0.274, -0.103, 0.011, 0.056, 0.016, 0.15, 0.182, 0.135, -0.273, -0.367, 0.127, 0.179, 0.052, 0.341, 0.046, 0.299, -0.129, 0.079, 0.035, 0.205, -0.062, 0.114, 0.062, 0.245, 0.16, 0.028, 0.109, 0.12, -0.239, -0.168, -0.171, -0.681, -0.336, 0.34, -0.002, 0.092, -0.188, -0.198, 0.119, 0.382, -0.034, 0.062, 0.02, 0.006, -0.126, 0.139, 0.195, 0.267, -0.015, 0.068, 0.166, 0.296, 0.176, 0.08, 0.167, 0.114, -0.078, 0.049, -0.182, -0.359, -0.585, -0.148, -0.302, 0.033, -0.313, -0.281, 0.139, 0.253, 0.083, -0.123, -0.106, 0.147, 0.043, 0.24, 0.106, 0.05, 0.017, 0.063, 0.182, 0.228, 0.119, 0.094, 0.068, 0.065, 0.033, 0.016, -0.036, 0.053, -0.458, -0.31, -0.068, -0.016, -0.249, -0.261, 0.269, 0.122, 0.306, 0.645, 0.253, -0.157, 0.101, 0.352, 0.059, 0.059, 0.06, -0.014, 0.148, 0.068, 0.106, 0.043, 0.01, 0.028, -0.061, -0.241, -0.359, -0.187, 0.135, -0.029, -0.091, -0.038, -0.423, -0.523, -0.11, -0.048, -0.374, 0.033, -0.041, -0.185, 0.157, 0.04, -0.114, -0.091, 0.087, 0.038, 0.112, 0.013, 0.011, 0.041, 0.033, -0.014, -0.007, -0.049, -0.192, -0.044, 0.318, 0.288, 0.221, 0.475, 0.194, -0.248, -0.194, 0.278, 0.011, 0.241, -0.018, -0.138, 0.104, -0.228, 0.194, 0.105, 0.147, -0.028, -0.01, -0.002, -0.014, -0.058, 0.0, -0.054, 0.324, 0.02, -0.157, 0.055, -0.205, -0.084, 0.019, 0.171, 0.261, 0.239, -0.249, -0.322, -0.326, -0.795, 0.091, -0.238, 0.005, -0.077, 0.198, -0.095, -0.101, -0.151, -0.017, -0.049, -0.076, -0.091, 0.006, 0.033, 0.141, -0.158, 0.295, 0.438, 0.309, 0.177, -0.167, -0.198, -0.188, -0.593, 0.632, -0.444, 0.1, -0.126, -0.081, -0.442, -0.127, -0.282, 0.242, -0.069, -0.073, -0.1, -0.047, 0.0, -0.015, -0.053, -0.053, 0.085, 0.151, 0.182, -0.047, 0.099, 0.482, 0.201, 0.527, -0.274, -1.246, -1.462, 0.116, 0.276, 0.575, 0.195, -0.433, -0.274, 0.095, 0.093, 0.187, -0.099, -0.085, -0.035, -0.008]
b: -1.421576346986597
accuracy on train: 0.9997777777777778
accuracy on test: 0.98
training vectors: 4500
testing vectors: 1000
Gaussian Kernel results:
     pcost       dcost       gap    pres   dres
 0: -1.6492e+02 -6.6070e+03  3e+04  2e+00  5e-15
 1: -1.1407e+02 -3.0686e+03  5e+03  2e-01  3e-15
 2: -1.0459e+02 -7.6952e+02  9e+02  3e-02  5e-15
 3: -1.2684e+02 -3.0203e+02  2e+02  6e-03  3e-15
 4: -1.4133e+02 -2.0101e+02  6e+01  1e-03  3e-15
 5: -1.4945e+02 -1.6809e+02  2e+01  2e-04  3e-15
 6: -1.5144e+02 -1.6280e+02  1e+01  3e-05  3e-15
 7: -1.5364e+02 -1.5728e+02  4e+00  7e-06  3e-15
 8: -1.5443e+02 -1.5550e+02  1e+00  1e-13  3e-15
 9: -1.5478e+02 -1.5492e+02  1e-01  1e-13  3e-15
10: -1.5483e+02 -1.5484e+02  4e-03  8e-14  3e-15
11: -1.5483e+02 -1.5483e+02  6e-05  8e-14  3e-15
Optimal solution found.
b for sklearn: -0.5174696361504186
number of support vectors for sklearn: 811
support vectors: 858
b: -0.5172969078760035
alpha: [1.03172394e-08 8.46262383e-08 6.89074995e-09 ... 4.68194567e-09
 4.61059766e-09 9.99999977e-01]
accuracy on train: 0.9984444444444445
accuracy on test: 0.989
